{
  "hash": "8824cd970b288a85249efcbe20b1f2d5",
  "result": {
    "markdown": "---\ntitle: \"Ensuring & Showcasing the Statistical Correctness of your R Package\"\nauthor: \"Hugo Gruson\"\ndate: \"2022-11-16\"\ncategories: [code quality, R package, testing]\nimage: \"testing_error.jpg\"\nformat:\n  html: \n    toc: true\n---\n\n\nWe're evolving in an increasingly data-driven world. And since critical decisions are taken based on results produced by data scientists and data analysts, they need to be be able to trust the tools they use:\n\n\n{{< tweet AlexCEngler 1092123648834658305 >}}\n\n\n\nIt is now increasingly common to add continuous integration to software packages and libraries, to ensure the code is not crashing, and that future updates don't change your code output (snapshot tests).\nBut one type of test still remains uncommon: tests for statistical correctness. That is, tests that ensure the algorithm implemented in your package actually produce the correct results.\nIt is likely that most statistical package authors run some tests on their own during development but there doesn't seem to be guidelines on how to test statistical correctness in a solid and standard way [^1].\n\n[^1]: But see the [\"testing statistical software\" post from Alex Hayes](https://www.alexpghayes.com/post/2019-06-07_testing-statistical-software/) where he presents his process to determine if he deems a statistical package trustworthy or not.\n\nIn this blog post, we explore various methods to ensure the correctness of your statistical package. Importantly, we argue that these tests should be part of your continuous integration system, to ensure your tools remains valid throughout its life, and to let users verify how you validate your package.\n\nThe approaches presented here are non-exclusive and should ideally all be added to your tests. However, they are presented in order of stringency and priority to implement. We also take a example of a function computing the centroid of a list of points to demonstrate how you would follow the recommendations presentation here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Compute the centroid of a set of points\n#'\n#' @coords Coordinates of the points as a list of vectors. Each element of the \n#'   list is a point.\n#'\n#' @returns A vector of coordinates of the same length of each element of \n#'   `coords`\n#'   \n#' @examples\n#' centroid(list(c(0, 1, 5, 3), c(8, 6, 4, 3), c(10, 2, 3, 7)))\n#' \ncentroid <- function(coords) {\n\n  # ...\n  # Skip all the necessary input checking for the purpose of this demo\n  # ...\n\n  coords_mat <- do.call(rbind, coords)\n  \n  return(colMeans(coords_mat))\n  \n}\n```\n:::\n\n\n# Compare your results to the reference implementation\n\nThe most straightforward and most solid way to ensure your implementation is valid is to compare your results to the results of the reference implementation. The reference implementation can be a package in another language, an example with toy data in the scientific article introducing the method, etc.\n\nFor example, the [`{gemma2}` R package](https://github.com/fboehm/gemma2), which re-implements the methods from [the GEMMA tool written in C++](https://github.com/genetics-statistics/GEMMA), [verifies that values produced by both tools match](https://github.com/fboehm/gemma2/blob/ea3052f8609622f17224fb8ec5fd83bd1bceb33e/tests/testthat/test_calc_sigma.R#L34-L37):\n\n```r\ntest_that(\"Results of gemma2 equal those of GEMMA v 0.97\", {\n  expect_equal(Sigma_ee, diag(c(18.559, 12.3672)), tolerance = 0.0001)\n  expect_equal(Sigma_uu, diag(c(82.2973, 41.9238)), tolerance = 0.0001)\n})\n```\n\nHowever, this approach cannot be used in all cases. Indeed, there may not be a reference implementation in your case. Or it might be difficult to replicate identical computations in the case of algorithm with stochasticity [^2].\n\n[^2]: Setting the random seed is not enough to compare implementations across programming languages because different languages use different kind of Random Number Generators.\n\nNote that even if a **reference** implementation doesn't exist, it is still good practice to compare your implementation to competing ones. Discrepancies might reveal a bug in your implementation or theirs but in any case, finding it out is beneficial to the community.\n\n::: {.callout-tip}\n## Example with `centroid()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(testthat)\n\ntest_that(\"centroid() in 1D produces the same results as mean()\", {\n\n  x <- list(1, 5, 3, 10, 5)\n\n  expect_identical(centroid(x), mean(unlist(x)))\n  \n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest passed ðŸŽŠ\n```\n:::\n:::\n\n\n:::\n\n# Compare to a theoretical upper or lower bound\n\nAn alternative strategy is to compare your result to theoretical upper or lower bound. This offers a weaker guarantee that your implementation and your results are correct but it can still allow you to detect important mistakes.\n\nYou can see a [real-life example of this kind of test in the `{finalsize}` R package](https://github.com/epiverse-trace/finalsize/blob/a710767b38a9242f15ab4dcf18b02fb5b0bcf24f/tests/testthat/test-newton_solver_vary_r0.R#L1-L13). `{finalsize}` computes the final proportion of infected in a heterogeneous population according to an SIR model. Theory predicts that the number of infections is maximal in a well-mixed population:\n\n```r\n# Calculates the upper limit of final size given the r0\n# The upper limit is given by a well mixed population\nupper_limit <- function(r0) {\n  f <- function(par) {\n    abs(1 - exp(-r0 * par[1]) - par[1])\n  }\n  opt <- optim(\n    par = 0.5, fn = f,\n    lower = 0, upper = 1,\n    method = \"Brent\"\n  )\n  opt\n}\n```\n\n::: {.callout-tip}\n## Example with `mymean()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_that(\"centroid() is inside the hypercube containing the data points\", {\n  \n  x <- list(c(0, 1, 5, 3), c(8, 6, 4, 3), c(10, 2, 3, 7))\n\n  expect_true(all(centroid(x) <= Reduce(pmax, x)))\n  expect_true(all(centroid(x) >= Reduce(pmin, x)))\n  \n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest passed ðŸŒˆ\n```\n:::\n:::\n\n\n:::\n\n# Verify that output is changing as expected when a single parameter varies\n\nAn even looser way to test statistical correctness would be to control that output varies as expected when you update some parameters. This could be for example, checking that the values you return increase when you increase or decrease one of your input parameters.\n\n::: {.callout-tip}\n## Example with `centroid()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_that(\"centroid() increases when coordinates from one point increase\", {\n  \n  x <- list(c(0, 1, 5, 3), c(8, 6, 4, 3), c(10, 2, 3, 7))\n  \n  y <- x\n  y[[1]] <- y[[1]] + 1 \n\n  expect_true(all(centroid(x) < centroid(y)))\n  \n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest passed ðŸŽ‰\n```\n:::\n:::\n\n\n:::\n\n# Conclusion\n\nIn this post, we've presented different methods to verify the statistical correctness of your statistical software. We would like to highlight one more time that it's important to run these tests are part of your regular integration system, instead of running them just once at the start of the development. This will prevent the addition of possible errors in the code and show users what specific checks you are doing. By doing so, you are transparently committing to the highest quality, instead of asking users to just rely on trust:\n\n\n{{< tweet hadleywickham 1092129977540231168 >}}\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}